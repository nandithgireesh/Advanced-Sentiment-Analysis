{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53712d5",
      "metadata": {
        "id": "d53712d5"
      },
      "outputs": [],
      "source": [
        "# Advanced Sentiment Analysis with Gradio\n",
        "\n",
        "# STEP 1: Install dependencies\n",
        "!pip install -q nltk wordcloud matplotlib seaborn scikit-learn gradio textblob\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from textblob import TextBlob\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data only if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# STEP 3: Enhanced dataset with more diverse examples\n",
        "sample_data = {\n",
        "    'review': [\n",
        "        'This movie was absolutely wonderful and breathtaking!',\n",
        "        'Terrible film, complete waste of time and money.',\n",
        "        'Great acting, compelling storyline, and excellent direction.',\n",
        "        'Boring, predictable plot with poor character development.',\n",
        "        'Amazing cinematography and outstanding direction throughout.',\n",
        "        'The script was a disaster and the acting was equally terrible.',\n",
        "        'An excellent thriller with stunning visuals and great suspense!',\n",
        "        'Completely underwhelming and overhyped movie with no substance.',\n",
        "        'A beautiful, emotional rollercoaster of a film with depth.',\n",
        "        'Worst acting I\\'ve ever seen in a film, truly awful.',\n",
        "        'Mediocre film with some good moments but overall disappointing.',\n",
        "        'Spectacular performances and incredible storytelling.',\n",
        "        'Not the best movie but has its entertaining moments.',\n",
        "        'Absolutely fantastic! One of the best films I\\'ve ever seen.',\n",
        "        'The movie was okay, nothing special but watchable.',\n",
        "        'Brilliant cinematography and powerful performances.',\n",
        "        'Dull and uninspiring with a confusing plot.',\n",
        "        'Outstanding film with excellent character development.',\n",
        "        'Waste of time, couldn\\'t even finish watching it.',\n",
        "        'Decent movie with good acting and reasonable plot.'\n",
        "    ],\n",
        "    'sentiment': [\n",
        "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
        "        'negative', 'positive', 'negative', 'positive', 'negative',\n",
        "        'negative', 'positive', 'neutral', 'positive', 'neutral',\n",
        "        'positive', 'negative', 'positive', 'negative', 'positive'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# STEP 4: Enhanced Text Preprocessing Class\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.contractions = {\n",
        "            \"won't\": \"will not\", \"can't\": \"cannot\", \"n't\": \" not\",\n",
        "            \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\",\n",
        "            \"'d\": \" would\", \"'m\": \" am\"\n",
        "        }\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        \"\"\"Expand contractions in text\"\"\"\n",
        "        for contraction, expansion in self.contractions.items():\n",
        "            text = text.replace(contraction, expansion)\n",
        "        return text\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        text = str(text).lower()\n",
        "        text = self.expand_contractions(text)\n",
        "        text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)  # Keep only letters and spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "        return text\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [\n",
        "            self.lemmatizer.lemmatize(word)\n",
        "            for word in tokens\n",
        "            if word not in self.stop_words and len(word) > 2\n",
        "        ]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# STEP 5: Enhanced EDA with better visualizations\n",
        "def perform_eda(df):\n",
        "    \"\"\"Perform comprehensive EDA\"\"\"\n",
        "    print(\"📊 Dataset Statistics:\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
        "    print(f\"Average review length: {df['review'].str.len().mean():.2f} characters\")\n",
        "\n",
        "    # Enhanced sentiment distribution plot\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.countplot(data=df, x='sentiment', palette='viridis')\n",
        "    plt.title(\"Sentiment Distribution\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    df['review_length'] = df['review'].str.len()\n",
        "    sns.boxplot(data=df, x='sentiment', y='review_length', palette='viridis')\n",
        "    plt.title(\"Review Length by Sentiment\")\n",
        "    plt.ylabel(\"Review Length (characters)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_enhanced_wordcloud(text_freq, title, colormap='viridis'):\n",
        "    \"\"\"Create enhanced word cloud with better styling\"\"\"\n",
        "    if not text_freq:\n",
        "        print(f\"No text available for {title}\")\n",
        "        return\n",
        "\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        max_words=100,\n",
        "        colormap=colormap,\n",
        "        width=800,\n",
        "        height=400,\n",
        "        relative_scaling=0.5,\n",
        "        min_font_size=10\n",
        "    ).generate_from_frequencies(text_freq)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# STEP 6: Enhanced Model Training with Hyperparameter Tuning\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "        self.vectorizer = None\n",
        "        self.best_model = None\n",
        "        self.models = {}\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepare and split data\"\"\"\n",
        "        df['processed_review'] = df['review'].apply(self.preprocessor.preprocess_text)\n",
        "        X = df['processed_review']\n",
        "        y = df['sentiment']\n",
        "\n",
        "        return train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    def create_pipelines(self):\n",
        "        \"\"\"Create ML pipelines with hyperparameter tuning\"\"\"\n",
        "        pipelines = {\n",
        "            'Logistic Regression': Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
        "                ('clf', LogisticRegression(max_iter=1000))\n",
        "            ]),\n",
        "            'Naive Bayes': Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
        "                ('clf', MultinomialNB())\n",
        "            ]),\n",
        "            'SVM': Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
        "                ('clf', SVC(kernel='linear', probability=True))\n",
        "            ]),\n",
        "            'Random Forest': Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(max_features=8000, ngram_range=(1,2))),\n",
        "                ('clf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "            ])\n",
        "        }\n",
        "        return pipelines\n",
        "\n",
        "    def train_models(self, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"Train multiple models and select the best one\"\"\"\n",
        "        pipelines = self.create_pipelines()\n",
        "        results = {}\n",
        "\n",
        "        print(\"🚀 Training Models...\")\n",
        "        for name, pipeline in pipelines.items():\n",
        "            print(f\"\\n🔍 Training {name}...\")\n",
        "\n",
        "            # Train model\n",
        "            pipeline.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "            print(f\"{name} - Test Accuracy: {accuracy:.4f} | CV Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})\")\n",
        "\n",
        "            results[name] = {\n",
        "                'model': pipeline,\n",
        "                'accuracy': accuracy,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'predictions': y_pred\n",
        "            }\n",
        "\n",
        "        # Select best model\n",
        "        best_model_name = max(results, key=lambda x: results[x]['cv_mean'])\n",
        "        self.best_model = results[best_model_name]['model']\n",
        "\n",
        "        print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "        print(f\"Best CV Score: {results[best_model_name]['cv_mean']:.4f}\")\n",
        "\n",
        "        return results, best_model_name\n",
        "\n",
        "    def plot_results(self, results, y_test, best_model_name):\n",
        "        \"\"\"Plot model comparison and confusion matrix\"\"\"\n",
        "        # Model comparison\n",
        "        model_names = list(results.keys())\n",
        "        accuracies = [results[name]['accuracy'] for name in model_names]\n",
        "        cv_means = [results[name]['cv_mean'] for name in model_names]\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.35\n",
        "        plt.bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8)\n",
        "        plt.bar(x + width/2, cv_means, width, label='CV Accuracy', alpha=0.8)\n",
        "        plt.xlabel('Models')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Model Performance Comparison')\n",
        "        plt.xticks(x, model_names, rotation=45)\n",
        "        plt.legend()\n",
        "\n",
        "        # Confusion matrix\n",
        "        plt.subplot(1, 3, 2)\n",
        "        best_predictions = results[best_model_name]['predictions']\n",
        "        cm = confusion_matrix(y_test, best_predictions)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=self.best_model.classes_,\n",
        "                   yticklabels=self.best_model.classes_)\n",
        "        plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "\n",
        "        # Feature importance (for tree-based models)\n",
        "        if hasattr(self.best_model.named_steps['clf'], 'feature_importances_'):\n",
        "            plt.subplot(1, 3, 3)\n",
        "            feature_names = self.best_model.named_steps['tfidf'].get_feature_names_out()\n",
        "            importances = self.best_model.named_steps['clf'].feature_importances_\n",
        "            top_indices = np.argsort(importances)[-10:]\n",
        "            plt.barh(range(len(top_indices)), importances[top_indices])\n",
        "            plt.yticks(range(len(top_indices)), [feature_names[i] for i in top_indices])\n",
        "            plt.title('Top 10 Feature Importances')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def predict_sentiment(self, text):\n",
        "        \"\"\"Predict sentiment with confidence and additional features\"\"\"\n",
        "        if not self.best_model:\n",
        "            return \"Model not trained yet!\"\n",
        "\n",
        "        # Get prediction\n",
        "        prediction = self.best_model.predict([text])[0]\n",
        "        probabilities = self.best_model.predict_proba([text])[0]\n",
        "        confidence = max(probabilities) * 100\n",
        "\n",
        "        # Additional features using TextBlob\n",
        "        blob = TextBlob(text)\n",
        "        textblob_sentiment = blob.sentiment.polarity\n",
        "\n",
        "        # Create detailed response\n",
        "        result = f\"🎯 *Predicted Sentiment:* {prediction.capitalize()}\\n\"\n",
        "        result += f\"📊 *Confidence:* {confidence:.2f}%\\n\"\n",
        "        result += f\"🔍 *TextBlob Polarity:* {textblob_sentiment:.3f}\\n\"\n",
        "\n",
        "        if confidence < 60:\n",
        "            result += \"⚠ *Note:* Low confidence prediction. The text might be ambiguous.\"\n",
        "\n",
        "        return result\n",
        "\n",
        "# STEP 7: Main execution\n",
        "def main():\n",
        "    # Initialize analyzer\n",
        "    analyzer = SentimentAnalyzer()\n",
        "\n",
        "    # Perform EDA\n",
        "    perform_eda(df)\n",
        "\n",
        "    # Prepare data\n",
        "    X_train, X_test, y_train, y_test = analyzer.prepare_data(df)\n",
        "\n",
        "    # Create word clouds\n",
        "    processed_df = df.copy()\n",
        "    processed_df['processed_review'] = processed_df['review'].apply(analyzer.preprocessor.preprocess_text)\n",
        "\n",
        "    from collections import Counter\n",
        "    for sentiment in df['sentiment'].unique():\n",
        "        sentiment_text = ' '.join(processed_df[processed_df['sentiment'] == sentiment]['processed_review'])\n",
        "        # Generate word frequencies\n",
        "        word_counts = Counter(sentiment_text.split())\n",
        "        create_enhanced_wordcloud(word_counts, f'{sentiment.capitalize()} Reviews WordCloud')\n",
        "\n",
        "\n",
        "    # Train models\n",
        "    results, best_model_name = analyzer.train_models(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Plot results\n",
        "    analyzer.plot_results(results, y_test, best_model_name)\n",
        "\n",
        "    # Print classification report\n",
        "    best_predictions = results[best_model_name]['predictions']\n",
        "    print(f\"\\n📈 Classification Report for {best_model_name}:\")\n",
        "    print(classification_report(y_test, best_predictions))\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "# Run main function\n",
        "analyzer = main()\n",
        "\n",
        "# STEP 8: Enhanced Gradio Interface\n",
        "def create_gradio_interface(analyzer):\n",
        "    \"\"\"Create an enhanced Gradio interface\"\"\"\n",
        "\n",
        "    def analyze_text(text):\n",
        "        if not text.strip():\n",
        "            return \"Please enter some text to analyze.\"\n",
        "        return analyzer.predict_sentiment(text)\n",
        "\n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 800px;\n",
        "        margin: auto;\n",
        "    }\n",
        "    .output-text {\n",
        "        font-family: 'Courier New', monospace;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=analyze_text,\n",
        "        inputs=[\n",
        "            gr.Textbox(\n",
        "                lines=5,\n",
        "                placeholder=\"Enter a movie review or any text to analyze sentiment...\",\n",
        "                label=\"📝 Input Text\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"📊 Sentiment Analysis Results\",\n",
        "                lines=6\n",
        "            )\n",
        "        ],\n",
        "        title=\"🎬 Advanced Sentiment Analysis Tool\",\n",
        "        description=\"Enter any text to get detailed sentiment analysis with confidence scores and additional insights.\",\n",
        "        examples=[\n",
        "            [\"This movie was absolutely amazing! The acting was superb and the plot was engaging.\"],\n",
        "            [\"I didn't like this film at all. It was boring and predictable.\"],\n",
        "            [\"The movie was okay, nothing special but watchable.\"],\n",
        "            [\"Terrible acting and poor storyline. Complete waste of time.\"],\n",
        "            [\"Outstanding performances and incredible cinematography made this a masterpiece.\"]\n",
        "        ],\n",
        "        css=custom_css,\n",
        "        theme=gr.themes.Soft()\n",
        "    )\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create and launch the interface\n",
        "interface = create_gradio_interface(analyzer)\n",
        "interface.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}